{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/solvedbrunus/Project-2-NLP/blob/main/nlp_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Manipulation:\n",
    "\n",
    "pandas: Provides data structures like DataFrames, which are useful for handling and processing structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "YMDjYTVwOidU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction:\n",
    "\n",
    "CountVectorizer: Converts a collection of text documents to a matrix of token counts.\n",
    "\n",
    "TfidfVectorizer: Converts a collection of raw documents to a matrix of TF-IDF features, which reflect the importance of words in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training:\n",
    "\n",
    "MultinomialNB: Implements the Multinomial Naive Bayes algorithm, which is suitable for classification with discrete features (like word counts for text classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation:\n",
    "\n",
    "accuracy_score: Calculates the accuracy of the model by comparing the predicted labels with the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nw2JRK_tOidW"
   },
   "source": [
    "# 2- Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umrsVcNrOidX"
   },
   "source": [
    "2a - Define the file path once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "U8B-qyzqOidY"
   },
   "outputs": [],
   "source": [
    "file_path = \"D:/OneDrive - Royal HaskoningDHV/920791/Pri 3/ironhack/nlp-project/Project-2-NLP/dataset/training_data_lowercase.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tigQZj_0OidY"
   },
   "source": [
    "2b- Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "8rRBe7EnOidY"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cWOU1bIOidY"
   },
   "source": [
    "2c- Install the chardet library detect the encoding programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "9NYQLNKrOidZ",
    "outputId": "0c7728f6-f597-473f-af88-af9b2176265d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet in c:\\users\\920791\\appdata\\local\\miniconda3\\lib\\site-packages (5.2.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install chardet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NRCRdAbOidZ"
   },
   "source": [
    "2d- Detect the encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VnrcaY-JOida",
    "outputId": "31357d8b-cec3-4f51-a531-37f4d283a6f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The detected encoding is: UTF-8-SIG\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "# Read the first few bytes of the file to detect the encoding\n",
    "with open(file_path, 'rb') as file:\n",
    "    raw_data = file.read(10000)\n",
    "    result = chardet.detect(raw_data)\n",
    "    encoding = result['encoding']\n",
    "    print(f\"The detected encoding is: {encoding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wU0zwvcBOida"
   },
   "source": [
    "2e- Load the dataset with the correct encoding to handle BOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "6QyrYWytOida"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path, encoding='utf-8-sig', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6o6HB8KOida"
   },
   "source": [
    "2f- Display the column names to verify they are correctly parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GJgQhysjOidb",
    "outputId": "441c2901-764a-4dc6-bb3c-f1f21996403c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([0], dtype='int64')\n",
      "\n",
      "\n",
      "This result means that the dataset currently has a single column named '0'. This might indicate that the columns were not correctly parsed or assigned during the data loading or processing steps.\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "if list(data.columns) == [0]:\n",
    "    print(\"This result means that the dataset currently has a single column named '0'. This might indicate that the columns were not correctly parsed or assigned during the data loading or processing steps.\")\n",
    "else:\n",
    "    print(f\"The dataset has the following columns: {list(data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kj-S_UjhOidb"
   },
   "source": [
    "2g- Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J6fmYA9zOidb",
    "outputId": "403c6737-8d81-4b74-a758-0f3dcf5cfb4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  0\\tdonald trump sends out embarrassing new yea...\n",
      "1  0\\tdrunk bragging trump staffer started russia...\n",
      "2  0\\tsheriff david clarke becomes an internet jo...\n",
      "3  0\\ttrump is so obsessed he even has obama‚s na...\n",
      "4  0\\tpope francis just called out donald trump d...\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "Between the results from step 2f and 2g we have identified that the dataset is not correctly divided between label and text, and that the label is incorporated on each sentence at the begining, therefore we have to separate the characters '0\\t' from the main sentences. For this reason re restart the process of loading the dataset and later splitting the sentences after the '0\\t' part, which will then become our new label column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbNSFMtOOidb"
   },
   "source": [
    "2h- Load the dataset with the correct encoding to handle BOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "WuUkVW_BOidb"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path, encoding='utf-8-sig', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzKaqgDhOidb"
   },
   "source": [
    "2i- Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9Dqfri9Oidc",
    "outputId": "0fed10da-ed22-44b1-8773-056865895d60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  0\\tdonald trump sends out embarrassing new yea...\n",
      "1  0\\tdrunk bragging trump staffer started russia...\n",
      "2  0\\tsheriff david clarke becomes an internet jo...\n",
      "3  0\\ttrump is so obsessed he even has obama‚s na...\n",
      "4  0\\tpope francis just called out donald trump d...\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCILa7kgOidc"
   },
   "source": [
    "2j- Separate the first part of the sentences with the separator '0\\t' and assign it as the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "vytbOb5OOidc"
   },
   "outputs": [],
   "source": [
    "data[['label', 'text']] = data[0].str.split('\\t', n=1, expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaeXYXLKOidc"
   },
   "source": [
    "2k- Drop the original column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "ZKZfCGUrOidc"
   },
   "outputs": [],
   "source": [
    "data = data.drop(columns=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULQ-WBKgOidc"
   },
   "source": [
    "2l- Display the first few rows of the dataset after removing the first part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FY4ZBMC5Oidc",
    "outputId": "55dfa4d6-a542-4623-b730-a8744c8e8653"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0     0  donald trump sends out embarrassing new year‚s...\n",
      "1     0  drunk bragging trump staffer started russian c...\n",
      "2     0  sheriff david clarke becomes an internet joke ...\n",
      "3     0  trump is so obsessed he even has obama‚s name ...\n",
      "4     0  pope francis just called out donald trump duri...\n",
      "\n",
      "\n",
      "The dataset has the following columns: ['label', 'text']\n"
     ]
    }
   ],
   "source": [
    "print(data.head())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "if list(data.columns) == [0]:\n",
    "    print(\"This result means that the dataset currently has a single column named '0'. This might indicate that the columns were not correctly parsed or assigned during the data loading or processing steps.\")\n",
    "else:\n",
    "    print(f\"The dataset has the following columns: {list(data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AMz6IRaOidc"
   },
   "source": [
    "# 3- Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3a- Check for missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vSC9CvddOidd",
    "outputId": "79f56463-af2e-40bf-c2b3-2157f251e5c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label    0\n",
      "text     0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "This result means that in the column named 'label', the amount of missing values is 0, and in the column named 'text', the amount of missing values is 0.\n"
     ]
    }
   ],
   "source": [
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"This result means that in the column named 'label', the amount of missing values is {missing_values['label']}, and in the column named 'text', the amount of missing values is {missing_values['text']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3b- Conditionally drop rows with missing values in the 'label' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "RN1itnOUOidd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values found in the 'label' column. No rows were dropped.\n"
     ]
    }
   ],
   "source": [
    "if missing_values['label'] > 0:\n",
    "    data = data.dropna(subset=['label'])\n",
    "\n",
    "    \n",
    "    print(\"Rows with missing values in the 'label' column have been dropped.\")\n",
    "else:\n",
    "    print(\"No missing values found in the 'label' column. No rows were dropped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3c- Check Data After Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "W6kG2G-rOidd",
    "outputId": "f7bfc3b3-42e3-4385-c041-8c2635271c91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label    0\n",
      "text     0\n",
      "dtype: int64\n",
      "  label                                               text\n",
      "0     0  donald trump sends out embarrassing new year‚s...\n",
      "1     0  drunk bragging trump staffer started russian c...\n",
      "2     0  sheriff david clarke becomes an internet joke ...\n",
      "3     0  trump is so obsessed he even has obama‚s name ...\n",
      "4     0  pope francis just called out donald trump duri...\n",
      "(34152, 2)\n",
      "\n",
      "\n",
      "This result means that the dataset has 34152 rows and 2 columns. The two columns are 'label' and 'text'.\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())\n",
    "print(data.head())\n",
    "print(data.shape)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"This result means that the dataset has {data.shape[0]} rows and {data.shape[1]} columns. The two columns are 'label' and 'text'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into training and testing sets. The training set is used to train the model, and the testing set is used to evaluate the model’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4a- Define the test size and random state as variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4b- Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "hi4jzv3dOide"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has been split into training and testing sets.\n",
      "X_train and y_train are the training data and labels, respectively.\n",
      "X_test and y_test are the testing data and labels, respectively.\n",
      "20.0% of the data is used for testing, and 80.0% is used for training.\n",
      "The random_state=42 ensures that the split is reproducible, meaning that every time you run the code with the same random_state value, you will get the same split of training and testing data. This is important for consistency and reliability in your results.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=test_size, random_state=random_state)\n",
    "\n",
    "print(\"The dataset has been split into training and testing sets.\")\n",
    "print(f\"X_train and y_train are the training data and labels, respectively.\")\n",
    "print(f\"X_test and y_test are the testing data and labels, respectively.\")\n",
    "print(f\"{test_size * 100}% of the data is used for testing, and {(1 - test_size) * 100}% is used for training.\")\n",
    "print(f\"The random_state={random_state} ensures that the split is reproducible, meaning that every time you run the code with the same random_state value, you will get the same split of training and testing data. This is important for consistency and reliability in your results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use two methods to convert the text data into numerical features: Count Vectorizer and TF-IDF Vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5a- Method 1: Count Vectorizer\n",
    "\n",
    "Converts text into a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "rV8MPHnfOide"
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5b- Method 2: TF-IDF Vectorizer\n",
    "\n",
    "Converts text into a matrix of TF-IDF features, which reflect the importance of words in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "8aRg1f1fOide"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6a- Using Count Vectorizer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "3Im8IsqCOidf"
   },
   "outputs": [],
   "source": [
    "model_count = MultinomialNB()\n",
    "model_count.fit(X_train_count, y_train)\n",
    "y_pred_count = model_count.predict(X_test_count)\n",
    "accuracy_count = accuracy_score(y_test, y_pred_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6b- Using TF-IDF Vectorizer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "t9abblflOidf"
   },
   "outputs": [],
   "source": [
    "model_tfidf = MultinomialNB()\n",
    "model_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6c- Compare the accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "gLfD7t7dOidf",
    "outputId": "e6f199df-96b2-4db2-86e1-b0d6b17af1e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best feature representation is Count Vectorizer with an accuracy of 0.9448.\n",
      "\n",
      "\n",
      "This means:\n",
      "\n",
      "Count Vectorizer: Using the word counts as features worked better than using TF-IDF scores.\n",
      "Accuracy: The model correctly identified whether the news is fake or real 94.48% of the time.\n"
     ]
    }
   ],
   "source": [
    "if accuracy_tfidf > accuracy_count:\n",
    "    best_representation = \"TF-IDF Vectorizer\"\n",
    "    best_accuracy = accuracy_tfidf\n",
    "else:\n",
    "    best_representation = \"Count Vectorizer\"\n",
    "    best_accuracy = accuracy_count\n",
    "\n",
    "print(f\"The best feature representation is {best_representation} with an accuracy of {best_accuracy:.4f}.\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "if best_representation == \"Count Vectorizer\":\n",
    "    print(\"This means:\\n\\nCount Vectorizer: Using the word counts as features worked better than using TF-IDF scores.\\nAccuracy: The model correctly identified whether the news is fake or real {:.2f}% of the time.\".format(best_accuracy * 100))\n",
    "else:\n",
    "    print(\"This means:\\n\\nTF-IDF Vectorizer: Using the TF-IDF scores as features worked better than using word counts.\\nAccuracy: The model correctly identified whether the news is fake or real {:.2f}% of the time.\".format(best_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
