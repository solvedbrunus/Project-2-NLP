{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/solvedbrunus/Project-2-NLP/blob/main/nlp_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0- Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Manipulation:\n",
    "\n",
    "pandas: Provides data structures like DataFrames, which are useful for handling and processing structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YMDjYTVwOidU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction:\n",
    "\n",
    "CountVectorizer: Converts a collection of text documents to a matrix of token counts.\n",
    "\n",
    "TfidfVectorizer: Converts a collection of raw documents to a matrix of TF-IDF features, which reflect the importance of words in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training:\n",
    "\n",
    "MultinomialNB: Implements the Multinomial Naive Bayes algorithm, which is suitable for classification with discrete features (like word counts for text classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation:\n",
    "\n",
    "accuracy_score: Calculates the accuracy of the model by comparing the predicted labels with the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the chardet library detect the encoding programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet in c:\\users\\920791\\appdata\\local\\miniconda3\\lib\\site-packages (5.2.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install chardet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nw2JRK_tOidW"
   },
   "source": [
    "# 1- Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umrsVcNrOidX"
   },
   "source": [
    "1a - Define the file path once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8B-qyzqOidY"
   },
   "outputs": [],
   "source": [
    "file_path = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tigQZj_0OidY"
   },
   "source": [
    "1b- Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8rRBe7EnOidY"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NRCRdAbOidZ"
   },
   "source": [
    "1c- Detect the encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VnrcaY-JOida",
    "outputId": "31357d8b-cec3-4f51-a531-37f4d283a6f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The detected encoding is: UTF-8-SIG\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "# Read the first few bytes of the file to detect the encoding\n",
    "with open(file_path, 'rb') as file:\n",
    "    raw_data = file.read(10000)\n",
    "    result = chardet.detect(raw_data)\n",
    "    encoding = result['encoding']\n",
    "    print(f\"The detected encoding is: {encoding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wU0zwvcBOida"
   },
   "source": [
    "1d- Load the dataset with the correct encoding to handle BOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6QyrYWytOida"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path, encoding='utf-8-sig', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6o6HB8KOida"
   },
   "source": [
    "1e- Display the column names to verify they are correctly parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GJgQhysjOidb",
    "outputId": "441c2901-764a-4dc6-bb3c-f1f21996403c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([0], dtype='int64')\n",
      "\n",
      "\n",
      "This result means that the dataset currently has a single column named '0'. This might indicate that the columns were not correctly parsed or assigned during the data loading or processing steps.\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "if list(data.columns) == [0]:\n",
    "    print(\"This result means that the dataset currently has a single column named '0'. This might indicate that the columns were not correctly parsed or assigned during the data loading or processing steps.\")\n",
    "else:\n",
    "    print(f\"The dataset has the following columns: {list(data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kj-S_UjhOidb"
   },
   "source": [
    "1f- Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J6fmYA9zOidb",
    "outputId": "403c6737-8d81-4b74-a758-0f3dcf5cfb4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  0\\tdonald trump sends out embarrassing new yea...\n",
      "1  0\\tdrunk bragging trump staffer started russia...\n",
      "2  0\\tsheriff david clarke becomes an internet jo...\n",
      "3  0\\ttrump is so obsessed he even has obama‚s na...\n",
      "4  0\\tpope francis just called out donald trump d...\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "Between the results from step 2f and 2g we have identified that the dataset is not correctly divided between label and text, and that the label is incorporated on each sentence at the begining, therefore we have to separate the characters '0\\t' from the main sentences. For this reason re restart the process of loading the dataset and later splitting the sentences after the '0\\t' part, which will then become our new label column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbNSFMtOOidb"
   },
   "source": [
    "1g- Load the dataset with the correct encoding to handle BOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WuUkVW_BOidb"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path, encoding='utf-8-sig', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzKaqgDhOidb"
   },
   "source": [
    "1h- Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9Dqfri9Oidc",
    "outputId": "0fed10da-ed22-44b1-8773-056865895d60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  0\\tdonald trump sends out embarrassing new yea...\n",
      "1  0\\tdrunk bragging trump staffer started russia...\n",
      "2  0\\tsheriff david clarke becomes an internet jo...\n",
      "3  0\\ttrump is so obsessed he even has obama‚s na...\n",
      "4  0\\tpope francis just called out donald trump d...\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCILa7kgOidc"
   },
   "source": [
    "1i- Separate the first part of the sentences with the separator '0\\t' and assign it as the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vytbOb5OOidc"
   },
   "outputs": [],
   "source": [
    "data[['label', 'text']] = data[0].str.split('\\t', n=1, expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaeXYXLKOidc"
   },
   "source": [
    "1j- Drop the original column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ZKZfCGUrOidc"
   },
   "outputs": [],
   "source": [
    "data = data.drop(columns=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULQ-WBKgOidc"
   },
   "source": [
    "1k- Display the first few rows of the dataset after removing the first part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FY4ZBMC5Oidc",
    "outputId": "55dfa4d6-a542-4623-b730-a8744c8e8653"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0     0  donald trump sends out embarrassing new year‚s...\n",
      "1     0  drunk bragging trump staffer started russian c...\n",
      "2     0  sheriff david clarke becomes an internet joke ...\n",
      "3     0  trump is so obsessed he even has obama‚s name ...\n",
      "4     0  pope francis just called out donald trump duri...\n",
      "\n",
      "\n",
      "The dataset has the following columns: ['label', 'text']\n"
     ]
    }
   ],
   "source": [
    "print(data.head())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "if list(data.columns) == [0]:\n",
    "    print(\"This result means that the dataset currently has a single column named '0'. This might indicate that the columns were not correctly parsed or assigned during the data loading or processing steps.\")\n",
    "else:\n",
    "    print(f\"The dataset has the following columns: {list(data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AMz6IRaOidc"
   },
   "source": [
    "# 2- Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a- Check for missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vSC9CvddOidd",
    "outputId": "79f56463-af2e-40bf-c2b3-2157f251e5c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label    0\n",
      "text     0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "This result means that in the column named 'label', the amount of missing values is 0, and in the column named 'text', the amount of missing values is 0.\n"
     ]
    }
   ],
   "source": [
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"This result means that in the column named 'label', the amount of missing values is {missing_values['label']}, and in the column named 'text', the amount of missing values is {missing_values['text']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b- Conditionally drop rows with missing values in the 'label' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "RN1itnOUOidd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values found in the 'label' column. No rows were dropped.\n"
     ]
    }
   ],
   "source": [
    "if missing_values['label'] > 0:\n",
    "    data = data.dropna(subset=['label'])\n",
    "\n",
    "    \n",
    "    print(\"Rows with missing values in the 'label' column have been dropped.\")\n",
    "else:\n",
    "    print(\"No missing values found in the 'label' column. No rows were dropped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2c- Check Data After Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "W6kG2G-rOidd",
    "outputId": "f7bfc3b3-42e3-4385-c041-8c2635271c91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label    0\n",
      "text     0\n",
      "dtype: int64\n",
      "  label                                               text\n",
      "0     0  donald trump sends out embarrassing new year‚s...\n",
      "1     0  drunk bragging trump staffer started russian c...\n",
      "2     0  sheriff david clarke becomes an internet joke ...\n",
      "3     0  trump is so obsessed he even has obama‚s name ...\n",
      "4     0  pope francis just called out donald trump duri...\n",
      "(34152, 2)\n",
      "\n",
      "\n",
      "This result means that the dataset has 34152 rows and 2 columns. The two columns are 'label' and 'text'.\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())\n",
    "print(data.head())\n",
    "print(data.shape)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"This result means that the dataset has {data.shape[0]} rows and {data.shape[1]} columns. The two columns are 'label' and 'text'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into training and testing sets. The training set is used to train the model, and the testing set is used to evaluate the model’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3a- Define the test size and random state as variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3b- Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "hi4jzv3dOide"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has been split into training and testing sets.\n",
      "X_train and y_train are the training data and labels, respectively.\n",
      "X_test and y_test are the testing data and labels, respectively.\n",
      "20.0% of the data is used for testing, and 80.0% is used for training.\n",
      "The random_state=42 ensures that the split is reproducible, meaning that every time you run the code with the same random_state value, you will get the same split of training and testing data. This is important for consistency and reliability in your results.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=test_size, random_state=random_state)\n",
    "\n",
    "print(\"The dataset has been split into training and testing sets.\")\n",
    "print(f\"X_train and y_train are the training data and labels, respectively.\")\n",
    "print(f\"X_test and y_test are the testing data and labels, respectively.\")\n",
    "print(f\"{test_size * 100}% of the data is used for testing, and {(1 - test_size) * 100}% is used for training.\")\n",
    "print(f\"The random_state={random_state} ensures that the split is reproducible, meaning that every time you run the code with the same random_state value, you will get the same split of training and testing data. This is important for consistency and reliability in your results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use two methods to convert the text data into numerical features: Count Vectorizer and TF-IDF Vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4a- Method 1: Count Vectorizer\n",
    "\n",
    "Converts text into a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "rV8MPHnfOide"
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4b- Method 2: TF-IDF Vectorizer\n",
    "\n",
    "Converts text into a matrix of TF-IDF features, which reflect the importance of words in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "8aRg1f1fOide"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5a- Using Count Vectorizer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "3Im8IsqCOidf"
   },
   "outputs": [],
   "source": [
    "model_count = MultinomialNB()\n",
    "model_count.fit(X_train_count, y_train)\n",
    "y_pred_count = model_count.predict(X_test_count)\n",
    "accuracy_count = accuracy_score(y_test, y_pred_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5b- Using TF-IDF Vectorizer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "t9abblflOidf"
   },
   "outputs": [],
   "source": [
    "model_tfidf = MultinomialNB()\n",
    "model_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5c- Compare the accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "gLfD7t7dOidf",
    "outputId": "e6f199df-96b2-4db2-86e1-b0d6b17af1e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best feature representation is Count Vectorizer with an accuracy of 0.9448.\n",
      "\n",
      "\n",
      "This means:\n",
      "\n",
      "Count Vectorizer: Using the word counts as features worked better than using TF-IDF scores.\n",
      "Accuracy: The model correctly identified whether the news is fake or real 94.48% of the time.\n"
     ]
    }
   ],
   "source": [
    "if accuracy_tfidf > accuracy_count:\n",
    "    best_representation = \"TF-IDF Vectorizer\"\n",
    "    best_accuracy = accuracy_tfidf\n",
    "else:\n",
    "    best_representation = \"Count Vectorizer\"\n",
    "    best_accuracy = accuracy_count\n",
    "\n",
    "print(f\"The best feature representation is {best_representation} with an accuracy of {best_accuracy:.4f}.\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "if best_representation == \"Count Vectorizer\":\n",
    "    print(\"This means:\\n\\nCount Vectorizer: Using the word counts as features worked better than using TF-IDF scores.\\nAccuracy: The model correctly identified whether the news is fake or real {:.2f}% of the time.\".format(best_accuracy * 100))\n",
    "else:\n",
    "    print(\"This means:\\n\\nTF-IDF Vectorizer: Using the TF-IDF scores as features worked better than using word counts.\\nAccuracy: The model correctly identified whether the news is fake or real {:.2f}% of the time.\".format(best_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating New Test Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Load the New Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a - Define the file path once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file_path = \"D:/OneDrive - Royal HaskoningDHV/920791/Pri 3/ironhack/Project-2-NLP/Project-2-NLP/dataset/test_data_news_only.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1b- Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(new_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1c- Detect the encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The detected encoding is: utf-8\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "# Read the first few bytes of the file to detect the encoding\n",
    "with open(new_file_path, 'rb') as file:\n",
    "    raw_data = file.read(10000)\n",
    "    result = chardet.detect(raw_data)\n",
    "    encoding = result['encoding']\n",
    "    print(f\"The detected encoding is: {encoding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1d- Load the dataset with the correct encoding to handle BOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(new_file_path, encoding='utf-8-sig', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1e- Display the column names to verify they are correctly parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([0, 1], dtype='int64')\n",
      "\n",
      "\n",
      "The dataset has the following columns: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(new_data.columns)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "if list(new_data.columns) == [0]:\n",
    "    print(\"This result means that the dataset currently has a single column named '0'. This might indicate that the columns were not correctly parsed or assigned during the data loading or processing steps.\")\n",
    "else:\n",
    "    print(f\"The dataset has the following columns: {list(new_data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1f- Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0                                                  1\n",
      "0  NaN                                               news\n",
      "1  0.0  Southside Chicago Blacks Fight Against Liberal...\n",
      "2  1.0  WIFE OF LIONS QUARTERBACK Matthew Stafford Jus...\n",
      "3  2.0  HEY CNN‚Ä¶REMEMBER OBAMA‚ÄôS Notorious ‚ÄúFrid...\n",
      "4  3.0  BREAKING NEWS: SEBASTIAN GORKA OUT‚Ä¶Are Ivank...\n"
     ]
    }
   ],
   "source": [
    "print(new_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "It looks like the new dataset has some issues with encoding and column names. We will address these issues step by step to make the dataset compatible with the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1g- Re-Load the dataset with the correct encoding\n",
    "\n",
    "Ensure the dataset is loaded with the correct encoding to handle special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(new_file_path, encoding='utf-8-sig', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1h- Rename Columns\n",
    "\n",
    "Rename the columns to match the structure of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.columns = ['index', 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1i- Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index                                               text\n",
      "0    NaN                                               news\n",
      "1    0.0  Southside Chicago Blacks Fight Against Liberal...\n",
      "2    1.0  WIFE OF LIONS QUARTERBACK Matthew Stafford Jus...\n",
      "3    2.0  HEY CNN‚Ä¶REMEMBER OBAMA‚ÄôS Notorious ‚ÄúFrid...\n",
      "4    3.0  BREAKING NEWS: SEBASTIAN GORKA OUT‚Ä¶Are Ivank...\n"
     ]
    }
   ],
   "source": [
    "print(new_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1j- Remove the First Row if it Contains Column Names\n",
    "\n",
    "Check if the first row contains column names and remove it if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'news' in new_data.iloc[0].values:\n",
    "    new_data = new_data.drop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1k- Check the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index                                               text\n",
      "1    0.0  Southside Chicago Blacks Fight Against Liberal...\n",
      "2    1.0  WIFE OF LIONS QUARTERBACK Matthew Stafford Jus...\n",
      "3    2.0  HEY CNN‚Ä¶REMEMBER OBAMA‚ÄôS Notorious ‚ÄúFrid...\n",
      "4    3.0  BREAKING NEWS: SEBASTIAN GORKA OUT‚Ä¶Are Ivank...\n",
      "5    4.0  First Grader ‚ÄúInvestigated‚Äù in Principal‚Ä...\n"
     ]
    }
   ],
   "source": [
    "print(new_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1l- Reset Index\n",
    "\n",
    "Reset the index to ensure it starts from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1m- Check the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index                                               text\n",
      "0    0.0  Southside Chicago Blacks Fight Against Liberal...\n",
      "1    1.0  WIFE OF LIONS QUARTERBACK Matthew Stafford Jus...\n",
      "2    2.0  HEY CNN‚Ä¶REMEMBER OBAMA‚ÄôS Notorious ‚ÄúFrid...\n",
      "3    3.0  BREAKING NEWS: SEBASTIAN GORKA OUT‚Ä¶Are Ivank...\n",
      "4    4.0  First Grader ‚ÄúInvestigated‚Äù in Principal‚Ä...\n"
     ]
    }
   ],
   "source": [
    "print(new_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1n- Handle Special Characters\n",
    "\n",
    "Replace special characters with their correct representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['text'] = new_data['text'].str.replace('‚Ä¶', '...')\n",
    "new_data['text'] = new_data['text'].str.replace('‚Äô', \"'\")\n",
    "new_data['text'] = new_data['text'].str.replace('‚Äú', '\"')\n",
    "new_data['text'] = new_data['text'].str.replace('‚Äù', '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1o- Display the Cleaned Data\n",
    "\n",
    "Check the first few rows of the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index                                               text\n",
      "0    0.0  Southside Chicago Blacks Fight Against Liberal...\n",
      "1    1.0  WIFE OF LIONS QUARTERBACK Matthew Stafford Jus...\n",
      "2    2.0  HEY CNN...REMEMBER OBAMA'S Notorious \"Friday N...\n",
      "3    3.0  BREAKING NEWS: SEBASTIAN GORKA OUT...Are Ivank...\n",
      "4    4.0  First Grader \"Investigated\" in Principal's Off...\n"
     ]
    }
   ],
   "source": [
    "print(new_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1p- Convert Text to Lowercase\n",
    "\n",
    "Convert all text to lowercase to maintain consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['text'] = new_data['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1q- Display the Cleaned Data\n",
    "\n",
    "Check the first few rows of the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index                                               text\n",
      "0    0.0  southside chicago blacks fight against liberal...\n",
      "1    1.0  wife of lions quarterback matthew stafford jus...\n",
      "2    2.0  hey cnn...remember obama's notorious \"friday n...\n",
      "3    3.0  breaking news: sebastian gorka out...are ivank...\n",
      "4    4.0  first grader \"investigated\" in principal's off...\n"
     ]
    }
   ],
   "source": [
    "print(new_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1r- Add a Placeholder Label Column\n",
    "\n",
    "Since the new dataset does not have labels, add a placeholder column for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['label'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1s- Reorder Columns\n",
    "\n",
    "Ensure the columns are in the correct order: label and text. The label column has replaced the index column. This is intentional because the index column is not needed for the analysis. The label column is added as a placeholder for future predictions, and the text column contains the news data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_data[['label', 'text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1t- Display the Cleaned Data\n",
    "\n",
    "Check the first few rows of the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0  None  southside chicago blacks fight against liberal...\n",
      "1  None  wife of lions quarterback matthew stafford jus...\n",
      "2  None  hey cnn...remember obama's notorious \"friday n...\n",
      "3  None  breaking news: sebastian gorka out...are ivank...\n",
      "4  None  first grader \"investigated\" in principal's off...\n"
     ]
    }
   ],
   "source": [
    "print(new_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Preprocess New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a- Check for missing values in the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label    9983\n",
      "text        0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "This result means that in the column named 'label', the amount of missing values is 9983, and in the column named 'text', the amount of missing values is 0.\n"
     ]
    }
   ],
   "source": [
    "missing_values = new_data.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"This result means that in the column named 'label', the amount of missing values is {missing_values['label']}, and in the column named 'text', the amount of missing values is {missing_values['text']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b- Conditionally drop rows with missing values in the 'label' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the label column is intentionally filled with None values as placeholders, we do not need to drop any rows based on missing values in the label column. Instead, we can proceed to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2c- Check Data After Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label    9983\n",
      "text        0\n",
      "dtype: int64\n",
      "  label                                               text\n",
      "0  None  southside chicago blacks fight against liberal...\n",
      "1  None  wife of lions quarterback matthew stafford jus...\n",
      "2  None  hey cnn...remember obama's notorious \"friday n...\n",
      "3  None  breaking news: sebastian gorka out...are ivank...\n",
      "4  None  first grader \"investigated\" in principal's off...\n",
      "(9983, 2)\n",
      "\n",
      "\n",
      "This result means that the dataset has 9983 rows and 2 columns. The two columns are 'label' and 'text'.\n"
     ]
    }
   ],
   "source": [
    "print(new_data.isnull().sum())\n",
    "print(new_data.head())\n",
    "print(new_data.shape)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"This result means that the dataset has {new_data.shape[0]} rows and {new_data.shape[1]} columns. The two columns are 'label' and 'text'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3a- Transform the New Text Data into Numerical Features Using the Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_transformed_count = count_vectorizer.transform(new_data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3b- Transform the New Text Data into Numerical Features Using the TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_transformed_tfidf = tfidf_vectorizer.transform(new_data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Predict Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4a- Predict Labels Using the Count Vectorizer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions_count = model_count.predict(new_data_transformed_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4b- Predict Labels Using the TF-IDF Vectorizer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions_tfidf = model_tfidf.predict(new_data_transformed_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4c- Add the Predictions to the New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['predicted_label_count'] = new_predictions_count\n",
    "new_data['predicted_label_tfidf'] = new_predictions_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4d- Display the First Few Rows of the New Data with Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text  \\\n",
      "0  None  southside chicago blacks fight against liberal...   \n",
      "1  None  wife of lions quarterback matthew stafford jus...   \n",
      "2  None  hey cnn...remember obama's notorious \"friday n...   \n",
      "3  None  breaking news: sebastian gorka out...are ivank...   \n",
      "4  None  first grader \"investigated\" in principal's off...   \n",
      "\n",
      "  predicted_label_count predicted_label_tfidf  \n",
      "0                     0                     0  \n",
      "1                     0                     0  \n",
      "2                     0                     0  \n",
      "3                     0                     0  \n",
      "4                     0                     0  \n"
     ]
    }
   ],
   "source": [
    "print(new_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
